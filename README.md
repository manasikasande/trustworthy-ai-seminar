# Seminar Project - Trustworthiness of Foundation Models

This repository contains the implementation of the project from the coursework of the seminar "Trustworthiness of Foundation Models", conducted at Saarland University in summer semester 2024. The seminar provides an overview of current research on three aspects of trustworthiness - security, content authenticity, and training-time robustness. The project involves implementing approaches discussed in the research papers from the course.

## Project Details

The project is divided into 2 parts - red teaming and watermarking.

### Part 1: Red teaming
Red teaming is an approach used for adversarial testing of LLMs, to discover prompts for which the LLM gives a toxic or harmful response. It is an automated approach, where a LLM itself is used to generate prompts which are then given as inputs to the LLM under consideration. The output is then classified as toxic or non-toxic with the help of a toxicity classifier. Implementation involves various red teaming strategies:
- Zero Shot generation [3]
- Few Shot generation [3]
- Feedback Loop In-Context Red Teaming (FLIRT) [2]

Target and Red-Teaming LLM used - GPT2-alpaca
Toxicity classifier used - Roberta toxicity classifier

### Part 2: Watermarking
Watermarking is a technique which helps us in distinguishing synthetic texts (texts generated by LLMs) from natural texts. Specific patterns and markers are embedded within the output of a LLM. At the same time, the challenge is to not diminish the quality of the output generated by the LLM, and also ensure that the watermark cannot be removed from the text.

The project contains an implementation of:
- watermarking generation and detection algorithm proposed by Kirchenbauer et al. [1]
- T5 span attack on the watermarking algorithm proposed by Kirchenbauer et al. [1]

## References

- [1] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. arXiv preprint arXiv:2301.10226, 2023.

- [2] Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. Flirt: Feedback loop in-context red teaming. arXiv preprint arXiv:2308.04265, 2023.

- [3] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.


